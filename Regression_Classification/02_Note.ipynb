{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "Following Notebook Summarizes the [Week 2](https://www.coursera.org/learn/machine-learning/home/week/2) of [Machine Learning](https://www.coursera.org/learn/machine-learning?)\n",
    "\n",
    "### Index \n",
    "- [Vectorization](#vectorization)\n",
    "- [Multiple Linear Regression](#multiple-linear-regression)\n",
    "- [Gradient Descent](#gradient-descent)\n",
    "- [Feature Scaling](#feature-scaling)\n",
    "- []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In order to speed up the process of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "$$\n",
    "f_{\\vec{w}b}(\\vec{x}) = \\vec{w} * \\vec{x} + b\n",
    "$$\n",
    "\n",
    "Following Equation above is the mathematical equation for calculating *Multiple Linear Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "As Explained in the previous note, the purpose of gradient descent is \n",
    "$$\n",
    "\\min_{\\vec{w}b} J(\\vec{w}b)\n",
    "\\\\\n",
    "$$\n",
    "To find the minimum value for $\\vec{w}$ and $b$, so that it minimizes the *Cost Function* $J(\\vec{w}b)$\n",
    "$$\n",
    "w_{n} = w_{n} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(f_{\\vec{w}b}(\\vec{x}^{(i)}) - y^{(i)}) x_{n}^{(i)}\n",
    "\\\\\n",
    "b = b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{\\vec{w}b}(\\vec{x}^{(i)}) - y^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Without proper scaling of the features, gradient descent will have harder time converging due to the shape of the cost function being elongated due to features having different range of values. \n",
    "\n",
    "**Mean Normalization**\n",
    "\n",
    "$$\n",
    "\\frac{x_{i} - \\mu}{max-min}\n",
    "$$\n",
    "$\\mu$ == Mean (Mu)\n",
    "\n",
    "*Mean Normalization* scales the numerical datasets so that the ranges of the values are from between $-1 \\leq x \\leq 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-Score Normalization**\n",
    "$$\n",
    "\\frac{x_{i} - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "$\\sigma$ == Standard Deviation(sigma)\n",
    "\n",
    "*Z-Score Normalization* Scales the dataset so that the numerical values are converted to ranges between $-3 \\leq x \\leq 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "$$\n",
    "z = f_{(\\vec{w},b)}(\\vec{x}) = \\vec{w} * \\vec{x} + b\n",
    "\\\\\n",
    "g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\\\\n",
    "\\frac{1}{1+e^{-(\\vec{w}*\\vec{x}+b)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Cost Function for logistic regression is different from that of *Linear Regression*. If you visualize logistic regression using the cost function used in Linear regression, it would return a cost function that has multiple *local minimas* which makes it harder to do *gradient descent* as displayed in the image below\n",
    "\n",
    "\n",
    "<img src=\"./LogisticReg_CostVis.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression *squared error cost* which is used in *Linear Regression* cannot be applied to *Logistic Regression* as the non-linear nature of the model will result in 'wiggly', non-convex cost function with many potential *local minima*\n",
    "\n",
    "$$\n",
    "L(f_{\\vec{w},b}(\\vec{x}^{(i)}),y^{(i)}) = \n",
    "  \\begin{array}{l}\n",
    "  -log(f_{\\vec{w},b}(\\vec{x}^{(i)}))\\\\\n",
    "  -log(1-f_{\\vec{w},b}(\\vec{x}^{(i)}))\n",
    "  \\end{array}%\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "*Regularization* is a way to maintain majority of the features instead of discarding them by applying heavy weights on the features so as to reduce the parameters.\n",
    "\n",
    "### Regularized Linear Regression\n",
    "\n",
    "$$\n",
    "J(\\vec{w},b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})^{2} + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_{j}^{2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3409ef4a337282d13679ec96c667699dc5fb30c5ff7a8068d7e26a8f562e91ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
