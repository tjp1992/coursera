{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "The following notebook summarizes the Second part of [Machine Learning Specialization](https://www.coursera.org/learn/advanced-learning-algorithms?courseSlug=advanced-learning-algorithms&showOnboardingModal=checkAndRedirect&specialization=machine-learning-introduction) which deals with *Neural Networks* and Training Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "To understand how [tensorflow](https://www.tensorflow.org/) library is compiling a neural network model and train the model. It is essential to understand the basic steps in which a *logistic regression* is carried out. The following steps are standard to how a machine learning model is trained.\n",
    "\n",
    "**Model Training steps for Logistic Regression**\n",
    "\n",
    "- Specify how to compute output given input `x` and parameters `w`, `b`(define model)\n",
    "$$\\\\f_{\\vec{w},b}(\\vec{x})=?$$\n",
    "```python\n",
    "''' Raw Python Code '''\n",
    " z= np.dot(w,x) +b\n",
    " f_x = 1/(1+np.exp(-z))\n",
    "''' Tensorflow Library '''\n",
    "model = Sequential([\n",
    "  Dense(...),\n",
    "  Dense(...),\n",
    "  Dense(...)\n",
    "])\n",
    "```\n",
    "- Specify loss and cost, by specifying the loss, it tells the model to train so that it reduces the loss of the specified function\n",
    "$$\n",
    "L(f_{\\vec{w},b}(\\vec{X}),y)\\\\\n",
    "J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}L(f_{\\vec{w},b}(\\vec{x}^{(i)}),y^{i})\n",
    "$$\n",
    "```python\n",
    "''' Raw Python Code '''\n",
    "loss = -y * np.log(f_x)\n",
    "-(1-y) * np.log(1-f_x)\n",
    "''' Tensorflow Library '''\n",
    "model.compile(loss=BinaryCrossentropy())\n",
    "```\n",
    "- Train on data to minimize \n",
    "$$J(\\vec{w},b)$$\n",
    "\n",
    "```python\n",
    "''' Raw Python Code '''\n",
    "w = w - alpha * dj_dw # dj_dw is the derivative\n",
    "b = b - alpha * dj_db\n",
    "''' Tensorflow Library '''\n",
    "model.fit(X,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "There are other alternatives to the activation function depending on the purpose of the model. \n",
    "\n",
    "*Classification*\n",
    "- sigmoid\n",
    "\n",
    "*Regression*\n",
    "- linear activation\n",
    "- ReLU(Non Negative Value)\n",
    "\n",
    "*Hidden Layer*\n",
    "It has been proven that the model performs better when applied with *ReLU* for hidden layers. Not only does the model perform better, but the gradient descent is much faster when applied with *ReLU*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Softmax is used when the model needs to classify more than two classes(binary). The mathematical equation for Softmax Regression is as following\n",
    "\n",
    "$$\n",
    "a_{j} = \\frac{e^{zj}}{\\sum_{k=1}^{N}e^{zk}} = P(y=j|\\vec{x})\n",
    "$$\n",
    "\n",
    "*When output layer with activation **softmax**, it is the same as doing **logistic Regression***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f1088fa484f5b54e382e6f3ab804b0f8e7491f37ff80b3b36194223562fb4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
