{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "Following Notebook Summarizes the [Week 3](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/3) of [Machine Learning](https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/1)\n",
    "\n",
    "This weeks content is an extension of the previous module of *Data Pipeline*, it addresses methods to optimize the model through hyperparameter tuning, and resource management.\n",
    "\n",
    "**Index**\n",
    "\n",
    "- [Dimensionality](#dimensionality)\n",
    "- [Manual Dimensional Reduction](#manual-dimensional-reduction)\n",
    "  - [Principal Component Analysis](#principal-component-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality\n",
    "\n",
    "**Problems caused by High Dimensional data**\n",
    "- More dimensions -> more features\n",
    "- Risk of overfitting our models\n",
    "- Distances grow more and more alike as more features causes data points to grow apart\n",
    "- No clear distinction between clustered objects\n",
    "- Concentration phenomenon for Euclidean distance\n",
    "\n",
    "**Technical issues of High Dimensionality**\n",
    "- Runtime and System memory requirements\n",
    "- Solutions take longer to reach global optima\n",
    "- More dimensions raise the likelihood of correlated features\n",
    "\n",
    "\n",
    "**Important Concept**\n",
    "- Curse of Dimensionality\n",
    "- Hughes Concept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Dimensional Reduction\n",
    "\n",
    "There are many different Dimensionality Reduction methods, but depending on different models it requires different methods.\n",
    "\n",
    "| Problem | Method | Example |\n",
    "| --- | --- | --- |\n",
    "| Classification | Maximize Separation among Classes | Linear Discriminant Analysis (LDA) |\n",
    "| Regression | Maximize Correlation between projected data and response variable | Partial Least Squares (PLS) |\n",
    "| Unsupervised | Retain as much data variance as possible | [Principal Component Analysis (PCA)](#principal-component-analysis) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis doesn't change the dimensionality of the data. The following steps are taken when PCA is carried out.\n",
    "\n",
    "- PCA rotates the samples so that they are aligned with the coordinate axes.\n",
    "- PCA Shifts samples so that they have a mean of zero.\n",
    "\n",
    "PCA uses first Principal Components to maximize the variance of projections and second PC orthagonal to the first PC maximize the remaining variance.\n",
    "\n",
    "**Dimensionality Reduction Resources**\n",
    "- [Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf)\n",
    "- [Independent Component Analysis](https://arxiv.org/pdf/1404.2986.pdf)\n",
    "- [PCA Extensions](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/) (**Must Read**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f1088fa484f5b54e382e6f3ab804b0f8e7491f37ff80b3b36194223562fb4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
