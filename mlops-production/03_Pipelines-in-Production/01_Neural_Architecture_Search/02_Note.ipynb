{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "Following Notebook Summarizes the [Week 3](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/3) of [Machine Learning](https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/1)\n",
    "\n",
    "This weeks content is an extension of the previous module of *Data Pipeline*, it addresses methods to optimize the model through hyperparameter tuning, and resource management.\n",
    "\n",
    "**Index**\n",
    "\n",
    "- [Dimensionality](#dimensionality)\n",
    "- [Manual Dimensional Reduction](#manual-dimensional-reduction)\n",
    "  - [Principal Component Analysis](#principal-component-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality\n",
    "\n",
    "**Problems caused by High Dimensional data**\n",
    "- More dimensions -> more features\n",
    "- Risk of overfitting our models\n",
    "- Distances grow more and more alike as more features causes data points to grow apart\n",
    "- No clear distinction between clustered objects\n",
    "- Concentration phenomenon for Euclidean distance\n",
    "\n",
    "**Technical issues of High Dimensionality**\n",
    "- Runtime and System memory requirements\n",
    "- Solutions take longer to reach global optima\n",
    "- More dimensions raise the likelihood of correlated features\n",
    "\n",
    "\n",
    "**Important Concept**\n",
    "- Curse of Dimensionality\n",
    "- Hughes Concept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Dimensional Reduction\n",
    "\n",
    "There are many different Dimensionality Reduction methods, but depending on different models it requires different methods.\n",
    "\n",
    "| Problem | Method | Example |\n",
    "| --- | --- | --- |\n",
    "| Classification | Maximize Separation among Classes | Linear Discriminant Analysis (LDA) |\n",
    "| Regression | Maximize Correlation between projected data and response variable | Partial Least Squares (PLS) |\n",
    "| Unsupervised | Retain as much data variance as possible | [Principal Component Analysis (PCA)](#principal-component-analysis) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis doesn't change the dimensionality of the data. The following steps are taken when PCA is carried out.\n",
    "\n",
    "- PCA rotates the samples so that they are aligned with the coordinate axes.\n",
    "- PCA Shifts samples so that they have a mean of zero.\n",
    "\n",
    "PCA uses first Principal Components to maximize the variance of projections and second PC orthagonal to the first PC maximize the remaining variance.\n",
    "\n",
    "**Dimensionality Reduction Resources**\n",
    "\n",
    "- [Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf)\n",
    "- [Independent Component Analysis](https://arxiv.org/pdf/1404.2986.pdf)\n",
    "- [PCA Extensions](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/) (**Must Read Content**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobile, IoT, and similar use cases\n",
    "\n",
    "Edge devices such as cellphones offer a lot of opportunity to utilize ML, from auto completion to facial recognition. However, generating real-time predictions can be done via hosting the model on a server, or embedding the model into the device. Both methods provide pros and cons that may be crucial component on deciding how the service is carried out.\n",
    "\n",
    "**Inference on the cloud/server**\n",
    "\n",
    "*Pros*\n",
    "\n",
    "- Lots of Compute Capacity\n",
    "- Scalable Hardware\n",
    "- Model complexity handled by the server\n",
    "- Easy to add new features and update the model\n",
    "- Low-latency and batch prediction\n",
    "\n",
    "*Cons*\n",
    "\n",
    "- Timely inference is needed\n",
    "- Constant connection to the device is required\n",
    "\n",
    "**On-Device Inference**\n",
    "\n",
    "*Pros*\n",
    "\n",
    "- Improved Speed\n",
    "- Performance\n",
    "- Network connectivty\n",
    "- No to-and-fro communication needed\n",
    "\n",
    "*Cons*\n",
    "\n",
    "- Less Capacity\n",
    "- Tight resource constraints\n",
    "\n",
    "There are different frameworks which provide faster and scalable model deployment.\n",
    "![img](./pics/model-deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "Quantization is process of reducing the required Computational Resource of a model by changing the weights of the models from floating points to integers. Usually this will result in faster computation speed, but reduced overall accuracy.\n",
    "\n",
    "There are two different methods to quantize a model, *post-quantization* and *quantization aware training*\n",
    "\n",
    "**Resources**\n",
    "- [Quantization](https://arxiv.org/abs/1712.05877)\n",
    "- [Post-Training Quantization](https://medium.com/tensorflow/introducing-the-model-optimization-toolkit-for-tensorflow-254aca1ba0a3)\n",
    "- [Quantization Aware Training](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "Pruning reduces the overall parameters and operations involving in network. \n",
    "\n",
    "\n",
    "\n",
    "**Resources**\n",
    "\n",
    "- [pruning](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)\n",
    "- [Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training\n",
    "\n",
    "There are two different types of distributed training, *Data Parallelism* and *Model Parallelism*.\n",
    "\n",
    "**Data Parallelism**\n",
    "Data Parallelism models are replicated into different accelerators and are given subset of a data for training.\n",
    "\n",
    "**Model Parallelism**\n",
    "Model Parallelism is used when a model is too large to be fit into a single device, the model is divided into partitions and fed into different accelerators.\n",
    "\n",
    "\n",
    "\n",
    "**Resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Performance Modeling\n",
    "\n",
    "**High Performance Ingestion**\n",
    "\n",
    "Accelerators are expensive and can be costly, which is why it is important to *High Performance Ingestion* for *High Performance Modeling*.\n",
    "\n",
    "One typical ETL pipeline that demonstrates a very inefficient pipeline is as following:\n",
    "\n",
    "<img src='./pics/typical_pipeline.png' width=\"700\" height=\"340\">\n",
    "\n",
    "It is inefficient because the pipeline is not efficiently using the *Computing Resource*(CPU, GPU) because there are idle times for CPU and GPU when training a model.\n",
    "\n",
    "The following demonstrates a more efficient pipeline that utilizes *Computing Resources* more efficiently. Though there are minute idle times between each training, it is significantly smaller compared to the typical pipeline demonstrated above. \n",
    "\n",
    "The following image shows parallelization of operations, by overlapping different parts of ETL using a technique known as **[Software Pipelineing](https://en.wikipedia.org/wiki/Software_pipelining)**\n",
    "\n",
    "<img src='./pics/efficient_pipeline.png' width=\"700\" height=\"340\">\n",
    "\n",
    "**How to optimize pipeline performances?**\n",
    "\n",
    "- Prefetching(Begin loading data for the next step before current finishes)\n",
    "- Parallelize data extraction and transformantion\n",
    "- Caching\n",
    "- Reduce Memory\n",
    "\n",
    "One framework allows for more efficient and scalable pipeline, it is `tf.data`, which allows for a more efficient ETL(Extract, Transform, Load) pipeline. \n",
    "\n",
    "**Resources**\n",
    "- [Distributed Training](https://www.tensorflow.org/guide/distributed_training)\n",
    "- [Data parallelism](https://arxiv.org/abs/1806.03377)\n",
    "- [Pipeline Parallelism](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html)\n",
    "- [GPipe](https://arxiv.org/abs/1811.06965)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3409ef4a337282d13679ec96c667699dc5fb30c5ff7a8068d7e26a8f562e91ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
