{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "Following Notebook Summarizes the [Week 2](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/2) of [Machine Learning](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops?)\n",
    "\n",
    "**Index**\n",
    "\n",
    "- [Preprocessing](#preprocessing-in-production)\n",
    "- [Feature Engineering](#feature-engineering-techniques)\n",
    "- [Preprocessing as Scale](#preprocessing-data-at-scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing in Production\n",
    "\n",
    "Preprocessing for feature engineering is a vital process to reduce the feature dimensions, and thus reducing the cost of the model(CPU, GPU). *Data preprocessing in essence is transformation of raw data into a clean and training-ready dataset*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Techniques\n",
    "\n",
    "There are many techniques for *Feature Engineering*, some of them are as following:\n",
    "\n",
    "- Numerical\n",
    "  - Normalization and Standardization\n",
    "  - Scaling\n",
    "- Grouping\n",
    "  - Bucketizing/Binning\n",
    "  - Bag of Words\n",
    "\n",
    "**Scaling**\n",
    "\n",
    "Scaling converts values from their natural range into a prescribed range\n",
    "\n",
    "\n",
    "$$\n",
    "  scaled(x) = \\frac{x-\\mu}{\\mu}\n",
    "$$\n",
    "\n",
    "*Benefits*\n",
    "\n",
    "- Helps Neural Nets converge faster\n",
    "- Do away with NaN errors during training\n",
    "- For each feature, the model learns the right weigths\n",
    "\n",
    "**Nomarlization**\n",
    "\n",
    "Normalization sets numerical values to range between [0,1] which allows faster computation, but requires the datasets to be not [Gaussian](https://en.wikipedia.org/wiki/Normal_distribution)(Indicating that the dataset is not Normally Distributed)  \n",
    "\n",
    "$$\n",
    "  X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}\n",
    "$$\n",
    "\n",
    "![Normalization](./pics/Normalization.png)\n",
    "\n",
    "**Standardization(Z-Score)**\n",
    "\n",
    "Z-score relates the number of standard deviations away from the mean.\n",
    "\n",
    "$$\n",
    "  X_{std} = \\frac{X-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "![Standardization](./pics/Standardization.png)\n",
    "\n",
    "**Bucketizing/Binning**\n",
    "\n",
    "Bucketizing is process of encoding numerical values into category using range.\n",
    "\n",
    "![Bucketizing](./pics/Bucketizing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Crosses\n",
    "\n",
    "*Feature Crosses* combine multiple features into a new feature. It encodes nonlinearity in the feature space, or encodes the same information in fewer features. \n",
    "\n",
    "*i.e.* Multiplying features(width, height) from housing data to get the house area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data at Scale\n",
    "\n",
    "Preprocessing in serving is different compared to training, as there are many factors that affect the performance of the model. *i.e.* in production, there are different costs allocated to the memory, os environment, programmindg language, differences in raw data size(In training there may be 100 megabytes of data, while there are terabytes of data flowing in serving). \n",
    "\n",
    "**Tensorflow Transfrom**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "There are three different methods to carrying out feature selection. \n",
    "\n",
    "- [Filter Methods](#filter-methods)\n",
    "- [Wrapper Methods](#wrapper-methods)\n",
    "- [Embedded Methods](#embedded-methods)\n",
    "\n",
    "Each method has it's own pros and cons, but the biggest difference in each of the method is the way features are selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Methods\n",
    "\n",
    "Filter methods use correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Methods\n",
    "\n",
    "- Forward Elimination\n",
    "- Backward Elimination\n",
    "- Recursive feature elimination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded Methods\n",
    "\n",
    "- L1 Regularization\n",
    "- Feature importance\n",
    "\n",
    "Embedded Methods indicate a feature selection method that is embedded in the Tensorflow library. It is more efficient compared to feature selection, as it can carry out the feature selection without other codes. In addition, it tends to reduce features so that it minimizes computation cost compared to feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3409ef4a337282d13679ec96c667699dc5fb30c5ff7a8068d7e26a8f562e91ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
